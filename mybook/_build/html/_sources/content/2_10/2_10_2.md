# 2.10.2 Interior Methods

The complexity of linear programming problems was for many years one of the most important open questions in theoretical computer science. Efforts were made to prove that Dantzig’s Simplex method would always stop sooner than $\left(\begin{array}{l}n \\ m\end{array}\right)$ iterations, but instead, problems were devised which drive the Simplex method through the combinatorial explosion of basic solutions. On the other hand, the linear programming problem did not seem to be NP-hard either. <br> 
The question was first answered in 1979 when the Russian mathematician Leonid B. Khachiyan published an algorithm for solving linear programming problems in polynomial time. Initial confusion over the importance of Khachiyan’s discovery arose for two reasons. First, his results appeared in a very short article in a Russian journal and went unnoticed for months because of its obscurity as well as the fact that the report was written in the Russian language. After some time, Eugene Lawler at the University of California at Berkeley brought the article to the attention of the computer science community. The explanation that Khachiyan himself presented was so abbreviated that mathematicians had little inkling of its content. Finally, through Lawler’s efforts, Khachiyan’s work was expanded upon (and the details of the proof reconstructed) by Gacs and Lovasz (1981), who not only filled in the gaps in the proof but improved on the efficiency of the algorithm. Only then was the new idea available to the general mathematics community for consideration and discussion. Almost nothing was known about Khachiyan himself, and it was generally assumed, even by Gacs and Lovasz, that he had never published any previous works. However, as it turns out, (Aspvall and Stone 1980) cite four publications by Khachiyan prior to his famous one in 1979. <br> 
The second misunderstanding arose because Khachiyan’s algorithm was designed for linear programming problems in which c, A, and b are integers. Careless reporters publicized incorrectly that Khachiyan had developed a polynomial-time algorithm for integer programming problems (such as the traveling salesman problem). Because this part of the story was untrue, there was skepticism concerning just what Khachiyan really had done. Major newspapers around the world contributed to the notoriety (but sadly not to the clarification) of this remarkable discovery. <br> 
Because linear programming problems had been suspected of having borderline complexity—neither being NP-hard nor having a polynomial algorithm—Khachiyan’s demonstration of a polynomial-time algorithm was somewhat surprising and of immense importance. Even George Dantzig, who developed the (worst-case exponential-time) Simplex algorithm, graciously offered the comment that, “A lot of people, including myself, 68 Operations Researchspent a lot of time looking for a polynomial-time algorithm for linear programming. I feel stupid that I didn’t see it” (Kolata 1979). <br>
Khachiyan’s method operates by defining a sequence of ellipsoids (ellipses in a multidimensional space), each smaller than the previous ellipsoid, and each containing the feasible region. The method generates a sequence of points $x_0, x_1, x_2, \dots ,$ which form the centers of the ellipsoids. At each iteration, if the center $x_k$ of the ellipsoid is infeasible, a hyperplane parallel to a violated constraint and passing through $x_k$ is used to cut the ellipsoid in half. One half is completely infeasible, but the other half contains the feasible region (if it exists), so a smaller ellipsoid is constructed that surrounds this half. Eventually, some $x_k$ will lie in the feasible region. <br> 
From a practical standpoint, Khachiyan’s **ellipsoid method** lacked the many years of fine-tuning that had been directed toward improving the efficiency of the Simplex method. Therefore, although it was a polynomial-time algorithm, in practice the Simplex method was the preferred method because typically it performed quite well, and software implementations were readily available. It should be noted, however, that whereas the computation time for the Simplex method is most strongly dependent on the number of constraints $m$, Khachiyan’s method is relatively insensitive to $m$ and more strongly dependent on the number of decision variables $n$. Thus, it was supposed at the time that Khachiyan’s ellipsoid method might eventually be superior, in practice, to the Simplex method for problems with numerous constraints. In any case, just five years later in 1984, yet another new method appeared. <br> 
Narendra Karmarkar, a young mathematician at AT&T Bell Laboratories, announced an algorithm for solving linear programming problems that was even more efficient than Khachiyan’s method. Karmarkar’s method is called an interior point method since it operates from within the polyhedron of feasible points of the linear programming problem. The algorithm uses a series of projective transformations in which the polyhedron is first made smoother (normalized), then an arbitrary point is selected which is re-mapped to the center, and a sphere is inscribed in the polyhedron. Then a new point is selected, near the edge of the sphere and in the direction of the optimal solution. The space is then transformed or warped again so that this new point is in the center. The process is repeated until the selected point is the optimal solution to the linear programming problem. Karmarkar’s method of projective transformations demonstrates a polynomial-time complexity bound for linear programming that was better than any previously known bound. <br>
Karmarkar’s original announcement claimed that his method was many times faster than the Simplex method. But since AT&T Bell Laboratories’ proprietary interests precluded disclosure of the details of its implementation, it was not at first possible to test Karmarkar’s claims. In fact, for several years, the scientific community remained somewhat annoyed because no one outside Bell Laboratories was in a position to duplicate Karmarkar’s computational experiments—and hence the traditional scientific peer review process could not take place. <br>
Whereas Karmarkar had claimed computation times 50 times faster than Simplex based codes, outside researchers were implementing Karmarkar’s method and observing computation times 50 times worse. Eventually, however, over the next 10 years, it became evident that by using special data structures, efficient methods for handling sparse matrices, and clever Cholesky factorization techniques, the performance of Karmarkar’s method could become quite competitive with Simplex implementations. <br> 
An important side effect of the controversy over the validity of Karmarkar’s claims is that it sparked a great deal of interest in examining and refining Simplex implementations. Consequently, there are now many very efficient implementations of both approaches. <br> 
An overview (Lustig et al. 1994) indicated that small problems, in which the sum of the number of decision variables plus the number of constraints is less than 2000, can generally be solved faster with the Simplex method. For medium sized problems, in which that sum is less than 10,000, Simplex and interior methods compete evenly. And there are several extremely large linear programming problems that have now been solved by interior point methodswhich have never been solved by any Simplex code. An increasing number of commercial software products contain both interior point methods and Simplex methods that can be used together or separately in solving large or difficult problems. Each of these approaches has its advantages, and hybrid software that combines these complementary methods constitutes a powerful computational tool for solving linear programming problems. <br> 
As the methods suggested originally by Karmarkar became more widely understood, numerous researchers made their own various contributions to the practical implementation of interior point algorithms. A very thorough summary of theoretical and implementational developments, as well as computational experimentation, may be found in a feature article by (Lustig et al. 1994). Bixby (1994) presents an enlightening description of commercial interior point methods, options, and performance on benchmark problem instances. Saigal (1995) is a comprehensive reference that includes a large section on interior point methods. Mitra et al. (1988) report experimental studies with hybrid interior/Simplex methods. Thus, the theoretical merits of Karmarkar’s new approach, which had never been doubted, have finally been balanced by considerable practical computational experience. As an illustration of this, recall that interior point methods must remain in the interior of the feasible region. Yet computational experience shows that choosing a step length that gets very close to (and nearly outside of) the boundary of the region is actually most efficient. So-called barrier parameters are used to control the interior search in the feasible region. <br> 
The interior and barrier methods were inspired by (and incorporate) many of the more general methods of nonlinear programming. It should be noted that interior point methods did not originate with Karmarkar; in fact, the approach had been used since the 1950s for nonlinear programming problems. However, Karmarkar can be credited with demonstrating that interior point methods could also be practical for solving linear programming problems. Therefore, a student who wishes to fully understand these methods might well begin by reading the introductory notions presented in *Chapter 5* on Nonlinear Optimization, and then be prepared to embark on a serious study of the mathematics and numerical analysis underlying general optimization procedures.