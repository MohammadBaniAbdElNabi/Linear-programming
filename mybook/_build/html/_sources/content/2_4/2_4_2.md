# 2.4.2 Solutions of Linear Systems

We now have a system of linear equations, ( $Ax = b $), consisting of *m* equations and *n* unknowns. The *n* unknowns include the original decision variables and any other variables that may have been introduced in order to achieve standard form.

It may be useful at this point to review the material in the Appendix on solving systems of linear equations. If a system of independent equations has any solution, then ($ m \leq n $). If ( $m = n$ ) (and if rank ($ A = m $) and ( $A $) is nonsingular), then there is the unique solution $x = A^{-1}b$. In this case, there is only one set of values for the ( $x_i $) that is not in violation of problem constraints. Optimization of an objective function is not an issue here because there is only one feasible solution.

When ($ m < n $), there are infinitely many solutions to the system of equations. In this case, we have ( $n - m$ ) *degrees of freedom* in solving the system. This means that we can arbitrarily assign any values to any ($n - m$) of the *n* variables, and then solve the *m* equations in terms of the remaining *m* unknowns.

A **basic solution** to the system of *m* equations and *n* unknowns is obtained by setting ($ n - m $) of the variables to zero, and solving for the remaining *m* variables. The *m* variables that are not set equal to zero are called **basic variables**, and the ($ n - m $) variables set to zero are **non-basic variables**. The number of basic solutions is just the number of ways we can choose ($ n - m $) variables (or *m* variables) from the set of *n* variables, and this number is given by:

<br />

$$
\binom{n}{n - m} = \binom{n}{m} = \frac{n!}{m!(n - m)!}
$$
<br />

Not all of the basic solutions satisfy all problem constraints and non-negativity constraints. Those that do not meet these requirements are **infeasible solutions**. The ones that do meet the restrictions are called **basic feasible solutions**. An **optimal basic feasible solution** is a basic feasible solution that optimizes the objective function.

The basic feasible solutions correspond precisely to the **extreme points** of the feasible region (as defined in our earlier discussion of graphical solutions). Because any optimal feasible solution is guaranteed to occur at an extreme point (and consequently is a basic feasible solution), the search for an optimal basic feasible solution could be carried out by an examination of the at most ( $\binom{n}{m} $) basic feasible solutions and a determination of which one yields the best objective function value.

The **Simplex method** performs such a search, but in a very efficient way. We define two extreme points of the feasible region (or two basic feasible solutions) as being **adjacent** if all but one of their basic variables are the same. Thus, a transition from one basic feasible solution to an adjacent basic feasible solution can be thought of as exchanging the roles of one basic variable and one non-basic variable.

The Simplex method performs a sequence of such transitions and thereby examines a succession of adjacent extreme points. A transition to an adjacent extreme point will be made only if by doing so the objective function is improved (or stays the same). It is a property of linear programming problems that this type of search will lead us to the discovery of an optimal solution (if one exists). The Simplex method is not only successful in this sense, but it is remarkably efficient because it succeeds after examining only a fraction of the basic feasible solutions.