# 2.10.1 Extensions to General Simplex Methods

The majority of commercial software for linear programming is based on the Revised Simplex method, and most implementations employ the product form inverse. For efficiency and accuracy on a computer, a variety of additional features may also be incorporated. We merely mention a few of them here, and the interested reader can obtain a more thorough understanding using the references cited at the end of the chapter. <br>
The method used for computing and maintaining tableau information has a strong bearing on the size of problem that can be successfully attempted. More complicated implementations require greater skill and effort but operate with greater speed so that larger problems can be solved. <br>
The explicit inverse method is straightforward and can be efficient and useful for problems involving a few hundred rows. The product form inverse allows for problems in the range of 1000 or so rows. For problems with tens of thousands of rows, LU decomposition techniques have been developed, for use both in the iteration phases and during **re-inversion** of the basis. In simple terms, any basis matrix B can be rewritten as the product of two triangular matrices, L and U where L is lower triangular (with zeros above the main diagonal) and U is upper triangular (with zeros below the diagonal). This format enables very efficient inverse computation and solution of the system. <br>
In a linear program with many variables, it is very time consuming to examine every non-basic variable at each iteration to determine the one to enter the basis. Many linear programming implementations do not go to the effort to select the non-basic variable corresponding to the *most negative* top row coefficient, but rather one corresponding to any negative coefficient (i.e., any variable that will improve the objective function). Although this strategy may increase the total number of iterations, it is actually a time-saving and very rational approach because the negative top row coefficients only specify a per-unit improvement in $z$, and not an absolute overall improvement. Thus any good entering variable can be quickly selected for the next basis. <br> 
In many linear programming models, there are **upper bound constraints** ($xj ≤ uj$) for some or all of the variables. Constraints such as these, as well as **generalized upper bounds** ($∑xj ≤ uj$), can be dealt with using a method, introduced by Dantzig and Van Slyke (1967), that handles these constraints implicitly without enlarging the basis. (Recall that for each explicit constraint, there must be a basic variable; therefore, any additional constraints generally contribute to the amount of work and storage required by the Revised Simplex method.) Handling upper bound constraints implicitly does take time, but practice has shown that this is an advantageous trade-off that serves to keep the problem size from increasing. <br>
Very large linear programming models often result in a constraint matrix A in which the non-zero elements appear in patterns or blocks. When a problem exhibits such a high degree 66 Operations Research of structure, it may be possible to apply a **decomposition** technique (Dantzig and Wolfe 1960). The original model is partitioned, and the subproblems are then solved individually. <br>
Not only do non-zero elements of A often appear in patterns, but more generally, we find the matrix A to be very **sparse**. A sparse matrix is one with a very large proportion of zero elements. A rule of thumb is that large linear programming models typically have only about 10% non-zero elements; some practitioners claim that 1%–5% is a more realistic range. This sparsity is not a surprising phenomenon when we consider that in any large organization, certain sets of products, people, or processes tend to operate in groups, and are therefore subject to *local* constraints. When such a problem is formulated, a sparse matrix results because each variable is involved in a relatively small number of the constraints. <br> 
In order to make better use of available memory, sparse matrices should be stored in some type of a compressed format, using methods such as those described by (Murtagh 1981). For example, each non-zero element could be stored along with an encoded form of its row and column indices. The term *super sparse* has been used to describe matrices that are not only sparse but in which many of the non-zero elements are the same. (e.g., in many applications, the vast majority of non-zero coefficients have a value of one.) In that case, each distinct value need be stored only once, and elements are found via a table of addresses into a table of distinct element values. Sparse matrix handling techniques have been shown to be worthwhile even if the coefficient matrix A is stored on a peripheral memory device. Because transfer time is slow relative to computation time, it is prudent to maintain such large data structures in as compact a form as possible. <br>
Round-off error is a natural consequence of using finite precision computing devices. As was pointed out in **Chapter 1**, this inability to store computed results exactly is particularly pronounced when we perform arithmetic operations on numeric values of very different magnitudes, where we are often unable to record that portion of a result contributed by the smaller value. In an attempt to remove the source of some of these numerical inaccuracies, most commercial linear programming systems apply some kind of scaling before beginning the Simplex method. Rows and columns of the matrix A may be multiplied by constants in order to make the largest element of each row and column the same (Murtagh 1981). To improve the condition of a matrix (and, therefore, obtain greater accuracy of its inverse), all the elements of A should be kept within a reasonable range, say within a factor of $10^6$ or $10^8$ of each other (Orchard-Hays 1968). More elaborate and specific mechanisms for scaling have been devised. In general, a healthy awareness of the limitations of computer arithmetic and numerical computation is essential in understanding and interpreting computed results. <br> 
In a problem of any practical size, the elimination of artificial variables from an initial solution can take a considerable amount of computation time. The term crashing refers generally to any kind of technique that gives the Simplex method a head start and eliminates some of the early iterations. **Crashing** sometimes consists of choosing a set of (nonartificial) non-basic variables to enter the basis and replace the artificial variables, even at the expense of temporarily degrading the objective function or making the solution infeasible (Cooper and Steinberg 1974). An even better way to give a boost to the Simplex method is to obtain, from the user or analyst, problem specific information about which variables are likely to be basic variables in a final solution. Many commercial systems (particularly those for larger powerful computers) provide a means for introducing such information along with other problem data. It may also be possible to restart Simplex iterations using solutions from previous (incomplete) attempts at optimization. <br> 
Many commercial systems contain algorithms for **sensitivity analysis** (also called **ranging procedures** or **postoptimality analysis**). These techniques are applied after the Simplex method has already produced an optimal solution. Sensitivity analysis allows the user to determine the effect that changes in various problem parameters would have on the optimal solution. Changes in the objective (cost/profit) coefficients and in the resource levels (right hand sides of constraints) are commonly dealt with; some systems consider the addition of decision variables to the original model, but most systems do not handle changes in the constraint coefficients or the addition of new constraints.
The relationship between sensitivity analysis and the dual to a linear programming model was described in **Section 2.8**. It is not uncommon for commercial software to include subroutines embodying a method known as the *dual Simplex** method. During sensitivity analysis, if problem parameters are changed, the current (optimal) solution may become infeasible. However, the problem is then **dual feasible**, and can be reoptimized using the dual Simplex algorithm.